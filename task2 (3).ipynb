{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKsXerQWBEJT",
        "outputId": "e58f3a31-d79c-41dc-efac-147ceb6b444f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=892254821f1f08d711dc1912143abc914e2c5decca86bed64e0a1baf7263a45f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.25.2)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535718 sha256=047d665fa46d8009cc5194f9034c8582354a3b9c47858df9719cb6780f66be5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install gym-super-mario-bros\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0iuRqE9dBHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Setup the environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros2-v1')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# CNN definition\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(3136, 512)\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((84, 84)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def preprocess(state):\n",
        "    state = Image.fromarray(state)\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "# Setup hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_net = DQN(1, len(SIMPLE_MOVEMENT)).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.00025)\n",
        "memory = deque(maxlen=10000)\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Training variables\n",
        "num_episodes = 10\n",
        "target_update = 10\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "steps_done = 0\n",
        "\n",
        "# Metrics\n",
        "total_rewards = []\n",
        "total_scores = []\n",
        "total_steps = []\n",
        "\n",
        "# Training loop\n",
        "training_start = time.time()\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = preprocess(state)\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    score = 0\n",
        "\n",
        "    while True:\n",
        "        steps_done += 1\n",
        "        steps += 1\n",
        "\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state.to(device)).max(1)[1].view(1, 1).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "        score = info['score']\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    total_scores.append(score)\n",
        "    total_steps.append(steps)\n",
        "\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f'Episode {episode}: Average Reward: {np.mean(total_rewards[-100:])}, Average Score: {np.mean(total_scores[-100:])}')\n",
        "\n",
        "training_end = time.time()\n",
        "training_time = training_end - training_start\n",
        "\n",
        "# Evaluation\n",
        "average_reward = np.mean(total_rewards)\n",
        "average_score = np.mean(total_scores)\n",
        "average_steps = np.mean(total_steps)\n",
        "\n",
        "print(f\"Average Reward: {average_reward}\")\n",
        "print(f\"Average Game Score: {average_score}\")\n",
        "print(f\"Average Steps Per Episode: {average_steps}\")\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZC840r2YYUL",
        "outputId": "f78bf3ea-1ff0-40d8-f064-73dc1a87ea01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Average Reward: 312.0, Average Score: 0.0\n",
            "Average Reward: 451.2\n",
            "Average Game Score: 130.0\n",
            "Average Steps Per Episode: 13749.3\n",
            "Training Time: 804.4357535839081 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Setup the environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros2-v1')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "def ppo_update(policy_net, optimizer, states, actions, log_probs_old, returns, advantages, epsilon_clip=0.2, c1=0.5, c2=0.01):\n",
        "    for _ in range(4):  # Perform multiple updates using the same batch\n",
        "        action_probs, state_values = policy_net(states)\n",
        "        dist = Categorical(action_probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        ratio = (log_probs - log_probs_old).exp()\n",
        "\n",
        "        # Clipped objective\n",
        "        surr1 = ratio * advantages\n",
        "        surr2 = torch.clamp(ratio, 1 - epsilon_clip, 1 + epsilon_clip) * advantages\n",
        "        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "        # Value loss\n",
        "        critic_loss = F.mse_loss(state_values.squeeze(-1), returns)\n",
        "\n",
        "        # Entropy bonus\n",
        "        entropy_bonus = dist.entropy().mean()\n",
        "\n",
        "        # Total loss\n",
        "        loss = actor_loss + c1 * critic_loss - c2 * entropy_bonus\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "def preprocess(state):\n",
        "    state = Image.fromarray(state)\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "# Setup hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_net = DQN(1, len(SIMPLE_MOVEMENT)).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.00025)\n",
        "memory = deque(maxlen=10000)\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Training variables\n",
        "num_episodes = 10\n",
        "target_update = 10\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "steps_done = 0\n",
        "\n",
        "# Metrics\n",
        "total_rewards = []\n",
        "total_scores = []\n",
        "total_steps = []\n",
        "\n",
        "# Training loop\n",
        "training_start = time.time()\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = preprocess(state)\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    score = 0\n",
        "\n",
        "    while True:\n",
        "        steps_done += 1\n",
        "        steps += 1\n",
        "\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state.to(device)).max(1)[1].view(1, 1).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "        score = info['score']\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    total_scores.append(score)\n",
        "    total_steps.append(steps)\n",
        "\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f'Episode {episode}: Average Reward: {np.mean(total_rewards[-100:])}, Average Score: {np.mean(total_scores[-100:])}')\n",
        "\n",
        "training_end = time.time()\n",
        "training_time = training_end - training_start\n",
        "\n",
        "# Evaluation\n",
        "average_reward = np.mean(total_rewards)\n",
        "average_score = np.mean(total_scores)\n",
        "average_steps = np.mean(total_steps)\n",
        "\n",
        "print(f\"Average Reward: {average_reward}\")\n",
        "print(f\"Average Game Score: {average_score}\")\n",
        "print(f\"Average Steps Per Episode: {average_steps}\")\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP1vWF0cdEPp",
        "outputId": "03e04eac-254d-4f06-dc7e-fa46e8eead45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Average Reward: 738.0, Average Score: 500.0\n",
            "Average Reward: 514.4\n",
            "Average Game Score: 260.0\n",
            "Average Steps Per Episode: 17558.7\n",
            "Training Time: 1053.961466550827 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Setup the environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros2-v1')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_channels, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc = nn.Linear(3136, 512)\n",
        "\n",
        "        self.policy = nn.Linear(512, num_actions)\n",
        "        self.value = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
        "\n",
        "        policy = F.softmax(self.policy(x), dim=-1)\n",
        "        value = self.value(x)\n",
        "\n",
        "        return policy, value\n",
        "def preprocess(state):\n",
        "    state = Image.fromarray(state)\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "# Setup hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_net = DQN(1, len(SIMPLE_MOVEMENT)).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.00025)\n",
        "memory = deque(maxlen=10000)\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Training variables\n",
        "num_episodes = 10\n",
        "target_update = 10\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "steps_done = 0\n",
        "\n",
        "# Metrics\n",
        "total_rewards = []\n",
        "total_scores = []\n",
        "total_steps = []\n",
        "\n",
        "# Training loop\n",
        "training_start = time.time()\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = preprocess(state)\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    score = 0\n",
        "\n",
        "    while True:\n",
        "        steps_done += 1\n",
        "        steps += 1\n",
        "\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state.to(device)).max(1)[1].view(1, 1).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "        score = info['score']\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    total_scores.append(score)\n",
        "    total_steps.append(steps)\n",
        "\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f'Episode {episode}: Average Reward: {np.mean(total_rewards[-100:])}, Average Score: {np.mean(total_scores[-100:])}')\n",
        "\n",
        "training_end = time.time()\n",
        "training_time = training_end - training_start\n",
        "\n",
        "# Evaluation\n",
        "average_reward = np.mean(total_rewards)\n",
        "average_score = np.mean(total_scores)\n",
        "average_steps = np.mean(total_steps)\n",
        "\n",
        "print(f\"Average Reward: {average_reward}\")\n",
        "print(f\"Average Game Score: {average_score}\")\n",
        "print(f\"Average Steps Per Episode: {average_steps}\")\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ],
      "metadata": {
        "id": "xJeDuyvIdZXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cda96e-097b-4ced-b8e4-8b441b921b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Average Reward: 379.0, Average Score: 600.0\n",
            "Average Reward: 422.5\n",
            "Average Game Score: 150.0\n",
            "Average Steps Per Episode: 14684.5\n",
            "Training Time: 857.2615840435028 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Setup the environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros2-v1')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# CNN definition for both policy and target networks\n",
        "class DDQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DDQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(3136, 512)\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((84, 84)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def preprocess(state):\n",
        "    state = Image.fromarray(state)\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "# Setup hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_net = DDQN(1, len(SIMPLE_MOVEMENT)).to(device)\n",
        "target_net = DDQN(1, len(SIMPLE_MOVEMENT)).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.00025)\n",
        "memory = deque(maxlen=10000)\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Training variables\n",
        "num_episodes = 10\n",
        "target_update = 10\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "steps_done = 0\n",
        "\n",
        "# Metrics\n",
        "total_rewards = []\n",
        "total_scores = []\n",
        "total_steps = []\n",
        "\n",
        "# Training loop\n",
        "training_start = time.time()\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = preprocess(state)\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    score = 0\n",
        "\n",
        "    while True:\n",
        "        steps_done += 1\n",
        "        steps += 1\n",
        "\n",
        "        # Select and perform an action\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state.to(device)).max(1)[1].view(1, 1).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Observe new state\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = preprocess(next_state)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization\n",
        "        if len(memory) > batch_size:\n",
        "            transitions = random.sample(memory, batch_size)\n",
        "            batch = list(zip(*transitions))\n",
        "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = batch\n",
        "\n",
        "            state_batch = torch.cat(state_batch)\n",
        "            action_batch = torch.tensor(action_batch).unsqueeze(1)\n",
        "            reward_batch = torch.tensor(reward_batch).float()\n",
        "            next_state_batch = torch.cat(next_state_batch)\n",
        "            done_batch = torch.tensor(done_batch).float()\n",
        "\n",
        "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "            state_action_values = policy_net(state_batch.to(device)).gather(1, action_batch.to(device))\n",
        "\n",
        "            # Compute V(s_{t+1}) for all next states.\n",
        "            next_state_values = target_net(next_state_batch.to(device)).max(1)[0].detach()\n",
        "            expected_state_action_values = (next_state_values * gamma * (1 - done_batch)) + reward_batch\n",
        "\n",
        "            # Compute Huber loss\n",
        "            loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "            # Optimize the model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_reward += reward\n",
        "        score = info['score']\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Update the target network\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    total_scores.append(score)\n",
        "    total_steps.append(steps)\n",
        "\n",
        "    # Decrement epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if episode % target_update == 0:\n",
        "        print(f'Episode {episode}: Average Reward: {np.mean(total_rewards[-target_update:])}, Average Score: {np.mean(total_scores[-target_update:])}')\n",
        "\n",
        "training_end = time.time()\n",
        "training_time = training_end - training_start\n",
        "\n",
        "# Evaluation\n",
        "average_reward = np.mean(total_rewards)\n",
        "average_score = np.mean(total_scores)\n",
        "average_steps = np.mean(total_steps)\n",
        "\n",
        "print(f\"Average Reward: {average_reward}\")\n",
        "print(f\"Average Game Score: {average_score}\")\n",
        "print(f\"Average Steps Per Episode: {average_steps}\")\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogSbq7Y0uh7V",
        "outputId": "733ba941-2651-44e1-9400-ae62abe883e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Average Reward: 396.0, Average Score: 0.0\n",
            "Average Reward: 396.0\n",
            "Average Game Score: 0.0\n",
            "Average Steps Per Episode: 19384.0\n",
            "Training Time: 1704.4357583522797 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}